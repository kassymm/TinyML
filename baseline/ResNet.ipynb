{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "arch-md",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "```\n",
    "Stem:    Conv(3→16, 3×3) + BN + ReLU             32×32\n",
    "Stage 1: BasicBlock(16→16) × 2                   32×32  [residual]\n",
    "Stage 2: BasicBlock(16→32, stride=2) +           16×16\n",
    "         BasicBlock(32→32)                       16×16  [residual]\n",
    "Stage 3: BasicBlock(32→40, stride=2) +           8×8\n",
    "         BasicBlock(40→40)                       8×8  [residual]\n",
    "Head:    GlobalAvgPool → Dropout(0.1) → Linear(40→10)\n",
    "```\n",
    "\n",
    "Each **BasicBlock** follows the pre-activation residual pattern:\n",
    "`Conv(3×3) + BN + ReLU + Conv(3×3) + BN` with an identity or 1×1 projection shortcut.\n",
    "Downsampling uses stride-2 convolution in the first block of each stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "imports-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 2.10.0 | Device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/sal/lib/python3.14/site-packages/torchvision/datasets/cifar.py:83: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n",
      "  entry = pickle.load(f, encoding=\"latin1\")\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.dirname(os.path.abspath('shared.py')))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from source import (\n",
    "    get_dataloaders, train, evaluate,\n",
    "    count_parameters, model_size_kb, print_summary,\n",
    "    evaluate_pytorch,\n",
    "    SEED,\n",
    ")\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "DEVICE = torch.device('cpu')\n",
    "print(f'PyTorch {torch.__version__} | Device: {DEVICE}')\n",
    "\n",
    "trainloader, testloader = get_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "model-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 99,706\n",
      "Model size: 393.0 KB\n"
     ]
    }
   ],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"Standard residual block: Conv-BN-ReLU-Conv-BN + shortcut.\"\"\"\n",
    "    def __init__(self, in_ch, out_ch, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(out_ch)\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, 1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = out + self.shortcut(x)\n",
    "        return self.relu(out)\n",
    "\n",
    "\n",
    "class TinyResNet(nn.Module):\n",
    "    \"\"\"Compact ResNet for CIFAR-10.\n",
    "\n",
    "    ~99K parameters. Three stages of BasicBlocks (2 blocks each) with channel\n",
    "    widths (16, 32, 40). Inspired by ResNet-20 but scaled down to fit <500 KB.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.stage1 = nn.Sequential(\n",
    "            BasicBlock(16, 16),\n",
    "            BasicBlock(16, 16),\n",
    "        )\n",
    "        self.stage2 = nn.Sequential(\n",
    "            BasicBlock(16, 32, stride=2),\n",
    "            BasicBlock(32, 32),\n",
    "        )\n",
    "        self.stage3 = nn.Sequential(\n",
    "            BasicBlock(32, 40, stride=2),\n",
    "            BasicBlock(40, 40),\n",
    "        )\n",
    "        self.pool    = nn.AdaptiveAvgPool2d(1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc      = nn.Linear(40, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "model = TinyResNet().to(DEVICE)\n",
    "\n",
    "n  = count_parameters(model)\n",
    "kb = model_size_kb(model)\n",
    "print(f'Parameters: {n:,}')\n",
    "print(f'Model size: {kb:.1f} KB')\n",
    "assert n < 125_000 and kb < 500, 'Constraint violated before training!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_acc = train(\n",
    "    model, trainloader, testloader, DEVICE,\n",
    "    save_path='best_resnet.pth',\n",
    ")\n",
    "print_summary(model, final_acc, label='TinyResNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85f3b021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "  Test Accuracy: 90.73%\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Load best model and final evaluation\n",
    "model.load_state_dict(torch.load(\"../models/best_resnet.pth\", map_location=\"cpu\", weights_only=True))\n",
    "final_acc = evaluate_pytorch(model, testloader, DEVICE)\n",
    "final_size = model_size_kb(model)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"  Test Accuracy: {final_acc:.2f}%\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-md",
   "metadata": {},
   "source": [
    "**Channel width (16→32→40):** Narrower than the original ResNet-20 (16→32→64) to\n",
    "respect the 125K parameter budget. The bottleneck stage uses 40 instead of 64 channels,\n",
    "saving ~28K parameters while retaining enough capacity to represent complex features.\n",
    "\n",
    "**Shortcut projection:** When channel width or stride changes between stages, a 1×1\n",
    "conv + BN projects the identity to the correct shape. This adds only ~0.5K parameters\n",
    "per stage transition."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
